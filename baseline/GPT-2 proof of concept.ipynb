{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from itertools import chain\n",
    "from transformers import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.chdir(\"/Users/mm06832/Documents/Classes/CS585/final_project/chatbot-with-personality/data\")\n",
    "df = pd.read_csv(\"processed_data_final.csv\", low_memory = False)\n",
    "df =  df[df.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_1_id</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>sentence_2_id</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>speaker_1_id</th>\n",
       "      <th>speaker_2</th>\n",
       "      <th>speaker_2_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>history</th>\n",
       "      <th>...</th>\n",
       "      <th>thriller</th>\n",
       "      <th>short</th>\n",
       "      <th>western</th>\n",
       "      <th>documentary</th>\n",
       "      <th>horror</th>\n",
       "      <th>animation</th>\n",
       "      <th>film-noir</th>\n",
       "      <th>music</th>\n",
       "      <th>war</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
       "      <td>L194</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "      <td>L195</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>u0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "      <td>L195</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "      <td>L196</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>u0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['Can we make this quick?  Roxanne Korrine and...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "      <td>L196</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "      <td>L197</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>u0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>['Can we make this quick?  Roxanne Korrine and...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
       "      <td>L198</td>\n",
       "      <td>Forget it.</td>\n",
       "      <td>L199</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>u0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
       "      <td>L200</td>\n",
       "      <td>Cameron.</td>\n",
       "      <td>L201</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>u0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221611</td>\n",
       "      <td>Your orders, Mr Vereker?</td>\n",
       "      <td>L666369</td>\n",
       "      <td>I'm to take the Sikali with the main column to...</td>\n",
       "      <td>L666370</td>\n",
       "      <td>DURNFORD</td>\n",
       "      <td>u9030</td>\n",
       "      <td>VEREKER</td>\n",
       "      <td>u9034</td>\n",
       "      <td>m616</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221612</td>\n",
       "      <td>I'm to take the Sikali with the main column to...</td>\n",
       "      <td>L666370</td>\n",
       "      <td>Lord Chelmsford seems to want me to stay back ...</td>\n",
       "      <td>L666371</td>\n",
       "      <td>DURNFORD</td>\n",
       "      <td>u9030</td>\n",
       "      <td>VEREKER</td>\n",
       "      <td>u9034</td>\n",
       "      <td>m616</td>\n",
       "      <td>['Your orders, Mr Vereker?']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221613</td>\n",
       "      <td>Lord Chelmsford seems to want me to stay back ...</td>\n",
       "      <td>L666371</td>\n",
       "      <td>I think Chelmsford wants a good man on the bor...</td>\n",
       "      <td>L666372</td>\n",
       "      <td>DURNFORD</td>\n",
       "      <td>u9030</td>\n",
       "      <td>VEREKER</td>\n",
       "      <td>u9034</td>\n",
       "      <td>m616</td>\n",
       "      <td>['Your orders, Mr Vereker?', \"I'm to take the ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221614</td>\n",
       "      <td>Well I assure you, Sir, I have no desire to cr...</td>\n",
       "      <td>L666520</td>\n",
       "      <td>And I assure you, you do not In fact I'd be ob...</td>\n",
       "      <td>L666521</td>\n",
       "      <td>DURNFORD</td>\n",
       "      <td>u9030</td>\n",
       "      <td>VEREKER</td>\n",
       "      <td>u9034</td>\n",
       "      <td>m616</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221615</td>\n",
       "      <td>And I assure you, you do not In fact I'd be ob...</td>\n",
       "      <td>L666521</td>\n",
       "      <td>So far only their scouts. But we have had repo...</td>\n",
       "      <td>L666522</td>\n",
       "      <td>DURNFORD</td>\n",
       "      <td>u9030</td>\n",
       "      <td>VEREKER</td>\n",
       "      <td>u9034</td>\n",
       "      <td>m616</td>\n",
       "      <td>['Well I assure you, Sir, I have no desire to ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221616 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence_1 sentence_1_id  \\\n",
       "0       Can we make this quick?  Roxanne Korrine and A...          L194   \n",
       "1       Well, I thought we'd start with pronunciation,...          L195   \n",
       "2       Not the hacking and gagging and spitting part....          L196   \n",
       "3       You're asking me out.  That's so cute. What's ...          L198   \n",
       "4       No, no, it's my fault -- we didn't have a prop...          L200   \n",
       "...                                                   ...           ...   \n",
       "221611                           Your orders, Mr Vereker?       L666369   \n",
       "221612  I'm to take the Sikali with the main column to...       L666370   \n",
       "221613  Lord Chelmsford seems to want me to stay back ...       L666371   \n",
       "221614  Well I assure you, Sir, I have no desire to cr...       L666520   \n",
       "221615  And I assure you, you do not In fact I'd be ob...       L666521   \n",
       "\n",
       "                                               sentence_2 sentence_2_id  \\\n",
       "0       Well, I thought we'd start with pronunciation,...          L195   \n",
       "1       Not the hacking and gagging and spitting part....          L196   \n",
       "2       Okay... then how 'bout we try out some French ...          L197   \n",
       "3                                              Forget it.          L199   \n",
       "4                                                Cameron.          L201   \n",
       "...                                                   ...           ...   \n",
       "221611  I'm to take the Sikali with the main column to...       L666370   \n",
       "221612  Lord Chelmsford seems to want me to stay back ...       L666371   \n",
       "221613  I think Chelmsford wants a good man on the bor...       L666372   \n",
       "221614  And I assure you, you do not In fact I'd be ob...       L666521   \n",
       "221615  So far only their scouts. But we have had repo...       L666522   \n",
       "\n",
       "       speaker_1 speaker_1_id speaker_2 speaker_2_id movie_id  \\\n",
       "0         BIANCA           u0   CAMERON           u2       m0   \n",
       "1         BIANCA           u0   CAMERON           u2       m0   \n",
       "2         BIANCA           u0   CAMERON           u2       m0   \n",
       "3         BIANCA           u0   CAMERON           u2       m0   \n",
       "4         BIANCA           u0   CAMERON           u2       m0   \n",
       "...          ...          ...       ...          ...      ...   \n",
       "221611  DURNFORD        u9030   VEREKER        u9034     m616   \n",
       "221612  DURNFORD        u9030   VEREKER        u9034     m616   \n",
       "221613  DURNFORD        u9030   VEREKER        u9034     m616   \n",
       "221614  DURNFORD        u9030   VEREKER        u9034     m616   \n",
       "221615  DURNFORD        u9030   VEREKER        u9034     m616   \n",
       "\n",
       "                                                  history  ... thriller short  \\\n",
       "0                                                      []  ...        0     0   \n",
       "1       ['Can we make this quick?  Roxanne Korrine and...  ...        0     0   \n",
       "2       ['Can we make this quick?  Roxanne Korrine and...  ...        0     0   \n",
       "3                                                      []  ...        0     0   \n",
       "4                                                      []  ...        0     0   \n",
       "...                                                   ...  ...      ...   ...   \n",
       "221611                                                 []  ...        0     0   \n",
       "221612                       ['Your orders, Mr Vereker?']  ...        0     0   \n",
       "221613  ['Your orders, Mr Vereker?', \"I'm to take the ...  ...        0     0   \n",
       "221614                                                 []  ...        0     0   \n",
       "221615  ['Well I assure you, Sir, I have no desire to ...  ...        0     0   \n",
       "\n",
       "       western documentary  horror  animation film-noir music war counts  \n",
       "0            0           0       0          0         0     0   0      3  \n",
       "1            0           0       0          0         0     0   0      3  \n",
       "2            0           0       0          0         0     0   0      3  \n",
       "3            0           0       0          0         0     0   0      1  \n",
       "4            0           0       0          0         0     0   0      3  \n",
       "...        ...         ...     ...        ...       ...   ...  ..    ...  \n",
       "221611       0           0       0          0         0     0   1      3  \n",
       "221612       0           0       0          0         0     0   1      3  \n",
       "221613       0           0       0          0         0     0   1      3  \n",
       "221614       0           0       0          0         0     0   1      2  \n",
       "221615       0           0       0          0         0     0   1      2  \n",
       "\n",
       "[221616 rows x 43 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### don't run yet, still WIP. start from below instead\n",
    "\n",
    "def build_inputs(persona, history, reply):\n",
    "    # Build our sequence by adding delimiters and concatenating\n",
    "\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                    for i, s in enumerate(sequence[1:])]\n",
    "    # Build our word, segments and position inputs from the sequence\n",
    "    words = list(chain(*sequence))                          # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                    for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))                      # position tokens\n",
    "    return words, segments, position, sequence\n",
    "\n",
    "def pad(x, padding):\n",
    "    return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "(words_1, words_2,\n",
    " segments_1, segments_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                   for x in (words_1, words_2,\n",
    "                                             segments_1, segments_2)]\n",
    "\n",
    "def processTrainingData(df, index):\n",
    "    \n",
    "    bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "    \n",
    "    # Let's define our contexts and special tokens\n",
    "    persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "               [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "    history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "               [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "    correct_reply = X_train.iloc[0]['sentence_1'].split()\n",
    "    incorrect_reply = X_train.iloc[2]['sentence_1'].split()\n",
    "    \n",
    "    words_1, segments_1, position_1, sequence_1 = build_inputs(persona, history, correct_reply)\n",
    "    words_2, segments_2, position_2, sequence_2 = build_inputs(persona, history, incorrect_reply)\n",
    "\n",
    "    # Tokenize words and segments embeddings:\n",
    "    words_1 = tokenizer.convert_tokens_to_ids(words_1) \n",
    "    words_2 = tokenizer.convert_tokens_to_ids(words_2)\n",
    "    segments_1 = tokenizer.convert_tokens_to_ids(segments_1)\n",
    "    segments_2 = tokenizer.convert_tokens_to_ids(segments_2)\n",
    "    \n",
    "    lm_targets_1 = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
    "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "    lm_targets_2 = [-1] * len(words_2)\n",
    "    \n",
    "    # Store the position of the last tokens for the next-sentence prediction loss\n",
    "    last_token_1 = len(words_1) - 1\n",
    "    last_token_2 = len(words_2) - 1\n",
    "\n",
    "    # Now we can pad reply and distractor inputs and targets to the same length\n",
    "    padding_length = max(len(words), len(words_2))\n",
    "    def pad(x, padding):\n",
    "        return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "    (words_1, words_2,\n",
    "     segments_1, segments_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                       for x in (words_1, words_2,\n",
    "                                                 segments_1, segments_2)]\n",
    "\n",
    "    (lm_targets_1, lm_targets_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>')) for x in (lm_targets_1, lm_targets_2)]\n",
    "    \n",
    "    batch_seq_len = 1\n",
    "\n",
    "    # And gather reply and distractor inputs to build the input tensors:\n",
    "    # words tokens\n",
    "    input_ids = torch.tensor([words_1, words_2]).unsqueeze(batch_seq_len - 1)\n",
    "    # segment tokens\n",
    "    token_type_ids = torch.tensor([segments_1, segments_2]).unsqueeze(batch_seq_len - 1)\n",
    "    # Last tokens location\n",
    "    mc_token_ids = torch.tensor([last_token_1, last_token_2], dtype=torch.long).unsqueeze(batch_seq_len - 1)\n",
    "    # Language modeling labels\n",
    "    lm_labels = torch.tensor([lm_targets_1, lm_targets_2], dtype=torch.long).unsqueeze(batch_seq_len - 1)\n",
    "    # Next-sentence prediction labels\n",
    "    mc_labels = torch.zeros(batch_seq_len, dtype=torch.long) # Gold reply is 1st (index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratify train test for conversations with more than one exchange to prevent target leakage\n",
    "df = df.assign(counts=df.groupby('index')['index'].transform('count'))\n",
    "df_1 = df[df['counts'] == 1]\n",
    "df_2 = df[df['counts'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(df_1.drop('sentence_2', axis=1), df_1['sentence_2'])\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(df_2.drop('sentence_2', axis=1), df_2['sentence_2'],stratify=df_2['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_1, X_train_2])\n",
    "X_test = pd.concat([X_test_1, X_test_2])\n",
    "y_train = pd.concat([y_train_1, X_train_2])\n",
    "y_test = pd.concat([y_test_1, y_test_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50262, 768)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS = {\"bos_token\": \"<bos>\", \"eos_token\": \"<eos>\", \"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\"], \"pad_token\": \"<pad>\"}\n",
    "\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our contexts and special tokens\n",
    "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "correct_reply = X_train.iloc[0]['sentence_1'].split()#[\"great\", \"to\", \"hear\"]\n",
    "incorrect_reply = X_train.iloc[2]['sentence_1'].split() #[\"i\", \"hate\", \"you\"]\n",
    "bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "\n",
    "def build_inputs(persona, history, reply):\n",
    "    # Build our sequence by adding delimiters and concatenating\n",
    "    \n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    # Build our word, segments and position inputs from the sequence\n",
    "    words = list(chain(*sequence))                          # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))                      # position tokens\n",
    "    return words, segments, position, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1, segments_1, position_1, sequence_1 = build_inputs(persona, history, correct_reply)\n",
    "words_2, segments_2, position_2, sequence_2 = build_inputs(persona, history, incorrect_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " 'i',\n",
       " 'like',\n",
       " 'playing',\n",
       " 'football',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'from',\n",
       " 'NYC',\n",
       " '.',\n",
       " '<speaker1>',\n",
       " 'hello',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " '<speaker2>',\n",
       " 'i',\n",
       " 'am',\n",
       " 'fine',\n",
       " 'thanks',\n",
       " '.',\n",
       " '<speaker1>',\n",
       " 'Almost',\n",
       " \"didn't\",\n",
       " 'recognize',\n",
       " 'you',\n",
       " 'with',\n",
       " 'your',\n",
       " 'hair',\n",
       " 'like',\n",
       " 'that.',\n",
       " 'How',\n",
       " 'you',\n",
       " 'been?',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize words and segments embeddings:\n",
    "words_1 = tokenizer.convert_tokens_to_ids(words_1) \n",
    "words_2 = tokenizer.convert_tokens_to_ids(words_2)\n",
    "segments_1 = tokenizer.convert_tokens_to_ids(segments_1)\n",
    "segments_2 = tokenizer.convert_tokens_to_ids(segments_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50259,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50260,\n",
       " 50261,\n",
       " 50261,\n",
       " 50261]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_targets_1 = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
    "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "lm_targets_2 = [-1] * len(words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the position of the last tokens for the next-sentence prediction loss\n",
    "last_token_1 = len(words_1) - 1\n",
    "last_token_2 = len(words_2) - 1\n",
    "\n",
    "# Now we can pad reply and distractor inputs and targets to the same length\n",
    "padding_length = max(len(words), len(words_2))\n",
    "def pad(x, padding):\n",
    "    return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "(words_1, words_2,\n",
    " segments_1, segments_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                   for x in (words_1, words_2,\n",
    "                                             segments_1, segments_2)]\n",
    "\n",
    "(lm_targets_1, lm_targets_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>')) for x in (lm_targets_1, lm_targets_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_seq_len = 1\n",
    "\n",
    "# And gather reply and distractor inputs to build the input tensors:\n",
    "# words tokens\n",
    "input_ids = torch.tensor([words_1, words_2]).unsqueeze(batch_seq_len - 1)\n",
    "# segment tokens\n",
    "token_type_ids = torch.tensor([segments_1, segments_2]).unsqueeze(batch_seq_len - 1)\n",
    "# Last tokens location\n",
    "mc_token_ids = torch.tensor([last_token_1, last_token_2], dtype=torch.long).unsqueeze(batch_seq_len - 1)\n",
    "# Language modeling labels\n",
    "lm_labels = torch.tensor([lm_targets_1, lm_targets_2], dtype=torch.long).unsqueeze(batch_seq_len - 1)\n",
    "# Next-sentence prediction labels\n",
    "mc_labels = torch.zeros(batch_seq_len, dtype=torch.long) # Gold reply is 1st (index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40.5495, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_ids, mc_token_ids=mc_token_ids, mc_labels = mc_labels, token_type_ids = token_type_ids, lm_labels = lm_labels)\n",
    "lm_loss = output[0]\n",
    "mc_loss = output[1] \n",
    "lm_scores = output[2]\n",
    "mc_scores = output[3]\n",
    "\n",
    "lm_coef = 2.0\n",
    "mc_coef = 1.0\n",
    "\n",
    "total_loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP - to load data for training \n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class ConvDataset(Dataset, ):\n",
    "    \"\"\"Movie dialogue conversation dataset.\"\"\"\n",
    "    \n",
    "    def build_inputs(persona, history, reply):\n",
    "        # Build our sequence by adding delimiters and concatenating\n",
    "\n",
    "        sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "        sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                        for i, s in enumerate(sequence[1:])]\n",
    "        # Build our word, segments and position inputs from the sequence\n",
    "        words = list(chain(*sequence))                          # word tokens\n",
    "        segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                        for i, s in enumerate(sequence) for _ in s]\n",
    "        position = list(range(len(words)))                      # position tokens\n",
    "        return words, segments, position, sequence\n",
    "\n",
    "    def pad(x, padding):\n",
    "        return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "    (words_1, words_2,\n",
    "     segments_1, segments_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                       for x in (words_1, words_2,\n",
    "                                                 segments_1, segments_2)]\n",
    "    \n",
    "    def __getTest__():\n",
    "        return (self.y_train, self.y_test)\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_file, low_memory = False)\n",
    "        df =  df[df.columns[1:]]\n",
    "        df = df.assign(counts=df.groupby('index')['index'].transform('count'))\n",
    "        \n",
    "        df_1 = df[df['counts'] == 1]\n",
    "        df_2 = df[df['counts'] != 1]\n",
    "        \n",
    "        X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(df_1.drop('sentence_2', axis=1), df_1['sentence_2'])\n",
    "        X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(df_2.drop('sentence_2', axis=1), df_2['sentence_2'],stratify=df_2['index'])\n",
    "        \n",
    "        self.X_train = pd.concat([X_train_1, X_train_2])\n",
    "        self.X_test = pd.concat([X_test_1, X_test_2])\n",
    "        self.y_train = pd.concat([y_train_1, X_train_2])\n",
    "        self.y_test = pd.concat([y_test_1, y_test_2])\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "                   [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "        history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "                   [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "        correct_reply = self.X_train.iloc[0]['sentence_1'].split()\n",
    "        incorrect_index = random.sample(set(range(len(self.X_train))) - set(idx))\n",
    "        incorrect_reply = self.X_train.iloc[incorrect_index]['sentence_1'].split()\n",
    "\n",
    "        words_1, segments_1, position_1, sequence_1 = build_inputs(persona, history, correct_reply)\n",
    "        words_2, segments_2, position_2, sequence_2 = build_inputs(persona, history, incorrect_reply)\n",
    "\n",
    "        # Tokenize words and segments embeddings:\n",
    "        words_1 = tokenizer.convert_tokens_to_ids(words_1) \n",
    "        words_2 = tokenizer.convert_tokens_to_ids(words_2)\n",
    "        segments_1 = tokenizer.convert_tokens_to_ids(segments_1)\n",
    "        segments_2 = tokenizer.convert_tokens_to_ids(segments_2)\n",
    "\n",
    "        lm_targets_1 = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
    "                 + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "        lm_targets_2 = [-1] * len(words_2)\n",
    "\n",
    "        # Store the position of the last tokens for the next-sentence prediction loss\n",
    "        last_token_1 = len(words_1) - 1\n",
    "        last_token_2 = len(words_2) - 1\n",
    "\n",
    "        # Now we can pad reply and distractor inputs and targets to the same length\n",
    "        padding_length = max(len(words), len(words_2))\n",
    "        def pad(x, padding):\n",
    "            return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "        (words_1, words_2,\n",
    "         segments_1, segments_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
    "                                           for x in (words_1, words_2,\n",
    "                                                     segments_1, segments_2)]\n",
    "\n",
    "        (lm_targets_1, lm_targets_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>')) for x in (lm_targets_1, lm_targets_2)]\n",
    "\n",
    "        batch_seq_len = 1\n",
    "\n",
    "        # And gather reply and distractor inputs to build the input tensors:\n",
    "        # words tokens\n",
    "        input_ids = torch.tensor([words_1, words_2]).unsqueeze(batch_seq_len - 1)\n",
    "        # segment tokens\n",
    "        token_type_ids = torch.tensor([segments_1, segments_2]).unsqueeze(batch_seq_len - 1)\n",
    "        # Last tokens location\n",
    "        mc_token_ids = torch.tensor([last_token_1, last_token_2], dtype=torch.long).unsqueeze(batch_seq_len - 1)\n",
    "        # Language modeling labels\n",
    "        lm_labels = torch.tensor([lm_targets_1, lm_targets_2], dtype=torch.long).unsqueeze(batch_seq_len - 1)\n",
    "        # Next-sentence prediction labels\n",
    "        mc_labels = torch.zeros(batch_seq_len, dtype=torch.long) # Gold reply \n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain only final fc layer for language modeling task\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model.lm_head.in_features\n",
    "model.lm_head = nn.Linear(num_ftrs, vocab_size)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "#optimizer_conv = optim.SGD(model.lm_head.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#optimizer_conv\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
